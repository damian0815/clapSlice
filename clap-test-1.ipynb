{
 "cells": [
  {
   "cell_type": "code",
   "id": "96fe6f91a0038bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:18:30.961104Z",
     "start_time": "2025-06-22T08:17:53.284596Z"
    }
   },
   "source": [
    "import os\n",
    "!pip install datasets transformers librosa soundfile numpy\"<\"2 \"torch==2.2.1\" torchaudio\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./venv/lib/python3.12/site-packages (3.6.0)\r\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.52.4)\r\n",
      "Requirement already satisfied: librosa in ./venv/lib/python3.12/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: soundfile in ./venv/lib/python3.12/site-packages (0.13.1)\r\n",
      "Requirement already satisfied: numpy<2 in ./venv/lib/python3.12/site-packages (1.26.4)\r\n",
      "Collecting torch==2.2.1\r\n",
      "  Obtaining dependency information for torch==2.2.1 from https://files.pythonhosted.org/packages/9e/4b/19527706ab1d1f91cdafe0160407a61e5865e74592ea911f92a81c583125/torch-2.2.1-cp312-none-macosx_10_9_x86_64.whl.metadata\r\n",
      "  Downloading torch-2.2.1-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (4.14.0)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch==2.2.1) (2025.3.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.12/site-packages (from datasets) (20.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.3.0)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.12/site-packages (from datasets) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.33.0)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./venv/lib/python3.12/site-packages (from librosa) (3.0.1)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in ./venv/lib/python3.12/site-packages (from librosa) (0.61.2)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.12/site-packages (from librosa) (1.15.3)\r\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./venv/lib/python3.12/site-packages (from librosa) (1.7.0)\r\n",
      "Requirement already satisfied: joblib>=1.0 in ./venv/lib/python3.12/site-packages (from librosa) (1.5.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.12/site-packages (from librosa) (5.2.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in ./venv/lib/python3.12/site-packages (from librosa) (1.8.2)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./venv/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\r\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./venv/lib/python3.12/site-packages (from librosa) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in ./venv/lib/python3.12/site-packages (from librosa) (1.1.1)\r\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.12/site-packages (from soundfile) (1.17.1)\r\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting torchaudio\r\n",
      "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/b5/5b/105c9d2ff5257262d97c3b35d4740d242b673557d577959e9aae025a9cab/torchaudio-2.2.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata\r\n",
      "  Downloading torchaudio-2.2.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.12/site-packages (from fsspec->torch==2.2.1) (3.12.13)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\r\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.8)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch==2.2.1) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch==2.2.1) (1.3.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (6.5.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch==2.2.1) (1.20.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading torch-2.2.1-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m150.8/150.8 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading torchaudio-2.2.1-cp312-cp312-macosx_10_13_x86_64.whl (3.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hInstalling collected packages: torch, torchaudio\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.2.2\r\n",
      "    Uninstalling torch-2.2.2:\r\n",
      "      Successfully uninstalled torch-2.2.2\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 2.2.2\r\n",
      "    Uninstalling torchaudio-2.2.2:\r\n",
      "      Successfully uninstalled torchaudio-2.2.2\r\n",
      "Successfully installed torch-2.2.1 torchaudio-2.2.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T09:38:27.446543Z",
     "start_time": "2025-06-22T09:37:58.250168Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset(\"ashraq/esc50\")\n",
    "audio = dataset[\"train\"][\"audio\"][-1][\"array\"]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# old",
   "id": "18041bcfe6594701"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:44:20.280340Z",
     "start_time": "2025-06-22T07:44:19.042400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import ClapModel\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\")"
   ],
   "id": "5edd2ed6fb484c3",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:22:14.198051Z",
     "start_time": "2025-06-22T07:22:10.552821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-fused\", device='cpu')\n",
    "output = audio_classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\n",
    "print(output)"
   ],
   "id": "ec0abf713287a7a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9998761415481567, 'label': 'Sound of a dog'}, {'score': 0.00012389849871397018, 'label': 'Sound of vaccum cleaner'}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7010166645050049, 'label': 'Sound of vaccum cleaner'},\n",
       " {'score': 0.2989833652973175, 'label': 'Sound of a dog'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": [
    "#audio.shape\n",
    "audio_classifier(audio[0:2205], candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\n"
   ],
   "id": "1da4f374d875c290"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:22:20.325737Z",
     "start_time": "2025-06-22T07:22:20.270891Z"
    }
   },
   "cell_type": "code",
   "source": "audio_classifier.feature_extractor.sampling_rate",
   "id": "e3a868d9c9638d2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:25:28.608828Z",
     "start_time": "2025-06-22T07:25:28.528744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "isinstance(audio, np.ndarray)\n",
    "#type(audio_classifier)\n",
    "features: transformers.feature_extraction_utils.BatchFeature = audio_classifier.feature_extractor(\n",
    "            [audio], \n",
    "    sampling_rate=audio_classifier.feature_extractor.sampling_rate, \n",
    "    return_tensors=\"pt\"\n",
    ")"
   ],
   "id": "2a344e4e72446a67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.feature_extraction_utils.BatchFeature"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c80cb4982b74426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:26:21.633373Z",
     "start_time": "2025-06-22T07:26:21.445318Z"
    }
   },
   "cell_type": "code",
   "source": "features.data['input_features'].shape\n",
   "id": "ce2b6d817ec1295f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1001, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T07:43:14.870993Z",
     "start_time": "2025-06-22T07:43:14.749849Z"
    }
   },
   "cell_type": "code",
   "source": "type(audio_classifier.feature_extractor), type(audio_classifier.model)",
   "id": "fdf666346d124a13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.clap.feature_extraction_clap.ClapFeatureExtractor,\n",
       " transformers.models.clap.modeling_clap.ClapModel)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T02:05:59.422574Z",
     "start_time": "2025-06-22T02:05:59.401517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import ZeroShotAudioClassificationPipeline\n",
    "from transformers import ClapModel\n",
    "\n",
    "audio_classifier: ZeroShotAudioClassificationPipeline\n",
    "model = audio_classifier.model: ClapAudioModel\n"
   ],
   "id": "bbac81d5c6f9ccd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClapModel(\n",
       "  (text_model): ClapTextModel(\n",
       "    (embeddings): ClapTextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ClapTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ClapTextLayer(\n",
       "          (attention): ClapTextAttention(\n",
       "            (self): ClapTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ClapTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ClapTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ClapTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ClapTextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (text_projection): ClapProjectionLayer(\n",
       "    (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (audio_model): ClapAudioModel(\n",
       "    (audio_encoder): ClapAudioEncoder(\n",
       "      (patch_embed): ClapAudioPatchEmbed(\n",
       "        (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (fusion_model): ClapAudioAFFBlock(\n",
       "          (local_att): Sequential(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (global_att): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (mel_conv2d): Conv2d(1, 96, kernel_size=(4, 12), stride=(4, 12))\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): ClapAudioStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x ClapAudioLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): ClapAudioAttention(\n",
       "                (self): ClapAudioSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ClapAudioSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): ClapAudioIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ClapAudioOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): ClapAudioPatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ClapAudioStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x ClapAudioLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): ClapAudioAttention(\n",
       "                (self): ClapAudioSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ClapAudioSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): ClapAudioIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ClapAudioOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): ClapAudioPatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): ClapAudioStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-5): 6 x ClapAudioLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): ClapAudioAttention(\n",
       "                (self): ClapAudioSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ClapAudioSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): ClapAudioIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ClapAudioOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): ClapAudioPatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): ClapAudioStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x ClapAudioLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): ClapAudioAttention(\n",
       "                (self): ClapAudioSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): ClapAudioSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): ClapAudioIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): ClapAudioOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (audio_projection): ClapProjectionLayer(\n",
       "    (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "3cb23ac2-7d44-42dc-8acf-0df92184c026",
   "metadata": {},
   "source": "# Setup"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:25:10.751672Z",
     "start_time": "2025-06-22T10:25:02.621683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device='mps'\n",
    "from transformers import ClapModel, ClapFeatureExtractor\n",
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\", use_safetensors=True).to(device)\n",
    "feature_extractor: ClapFeatureExtractor = ClapFeatureExtractor.from_pretrained(\"laion/clap-htsat-unfused\")\n"
   ],
   "id": "5a78d7169b407d7d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run",
   "id": "6d42f4e095b6da6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:25:11.546842Z",
     "start_time": "2025-06-22T10:25:10.753206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "waveform, sampling_rate = torchaudio.load(\"/Users/damian/Downloads/green-day-when-i-come-around-4k-upgrade.mp3\")\n",
    "print(sampling_rate, waveform.shape)\n",
    "#inputs = feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "#audio_features = model.get_audio_features(**inputs)"
   ],
   "id": "d5d30b5e7ea5cb98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 torch.Size([2, 8631136])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:04:42.866256Z",
     "start_time": "2025-06-22T10:04:42.257016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = feature_extractor(waveform[0, 0:48000], sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "audio_features = model.get_audio_features(**inputs)\n",
    "audio_features"
   ],
   "id": "68c90d1df492b825",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8695e-03, -1.0465e-02,  5.2422e-03, -6.0487e-02,  4.6928e-03,\n",
       "          5.2191e-02, -1.4813e-02, -2.1933e-03,  6.5858e-02, -3.7286e-03,\n",
       "         -2.1416e-02, -3.3354e-02,  1.0374e-02,  1.1024e-02,  3.1634e-02,\n",
       "          2.3485e-02,  2.6002e-02, -5.8393e-02,  9.3072e-03,  4.0768e-02,\n",
       "          1.3185e-02,  3.3300e-02, -4.9509e-02, -1.5833e-01,  6.1356e-03,\n",
       "          3.3087e-02, -1.0351e-02, -6.5548e-02,  3.5765e-02, -4.5352e-02,\n",
       "         -2.6277e-02,  4.6952e-02,  1.5202e-02, -1.5501e-02,  3.8304e-04,\n",
       "         -1.7497e-02, -6.0437e-02,  4.3994e-03, -6.8155e-02, -5.6639e-02,\n",
       "         -2.4169e-02,  4.3117e-02,  5.2147e-03, -1.5196e-02,  9.7841e-03,\n",
       "         -6.5523e-02,  4.3005e-02,  6.0902e-02,  2.5703e-03, -7.8369e-02,\n",
       "          7.6009e-02,  8.4996e-03, -2.1377e-02, -4.9307e-02,  1.1909e-01,\n",
       "         -6.4489e-02,  8.6670e-02,  3.0398e-02, -6.8697e-02,  7.8314e-02,\n",
       "          4.8507e-02, -6.3483e-03, -1.9889e-02, -8.8189e-03,  9.0872e-02,\n",
       "          1.2882e-02, -6.9134e-02,  4.4477e-02,  2.0317e-02,  1.9113e-02,\n",
       "          3.3545e-02, -9.9285e-04, -1.0187e-01, -7.0741e-03,  1.1303e-02,\n",
       "          8.5205e-02,  8.4389e-02,  1.3961e-02,  3.8579e-02,  1.5127e-02,\n",
       "         -8.8349e-04, -2.2960e-02,  2.5316e-02, -4.2518e-03,  7.3695e-02,\n",
       "          6.6627e-03,  2.3508e-04,  1.6043e-03, -1.1329e-01, -1.0016e-01,\n",
       "         -5.2517e-02, -7.4676e-02,  4.4126e-02,  2.4389e-02,  7.7866e-02,\n",
       "          4.3450e-02, -5.1326e-02,  6.1874e-02, -1.5422e-02, -7.2359e-03,\n",
       "          8.9068e-03, -3.3409e-02,  8.1331e-02, -3.8393e-02, -3.3060e-02,\n",
       "         -2.9612e-02,  6.9287e-03, -8.7674e-02,  4.0557e-02,  5.1007e-02,\n",
       "         -1.5597e-02, -2.0053e-02, -1.4490e-02,  3.5561e-02, -2.3149e-03,\n",
       "         -4.0700e-03, -4.8429e-03,  3.5248e-02,  3.7231e-02, -5.9344e-03,\n",
       "          6.2536e-03, -3.8370e-04, -4.9737e-03,  2.1460e-02,  1.9881e-02,\n",
       "          8.0969e-02,  3.8459e-02, -2.6547e-02, -3.3438e-02, -3.5131e-02,\n",
       "         -1.3894e-02, -1.0839e-02, -6.4992e-02,  5.1061e-04, -9.3806e-02,\n",
       "         -7.2494e-02, -1.1145e-03,  7.5564e-02,  4.3212e-02, -2.6650e-02,\n",
       "          1.5646e-03,  2.9608e-02, -1.1682e-03, -6.8320e-02, -4.8814e-02,\n",
       "          2.9369e-03, -1.4178e-02,  1.2747e-01,  8.3179e-03,  1.0247e-03,\n",
       "          2.3362e-02, -3.2777e-02,  3.2210e-02,  3.0943e-02,  1.0107e-02,\n",
       "          6.0315e-02, -1.8556e-02, -6.7305e-02,  1.5482e-02,  2.2471e-02,\n",
       "          1.0490e-01, -3.4195e-03, -3.9124e-02,  5.1390e-02, -2.6663e-02,\n",
       "         -9.3637e-02, -4.4379e-02, -2.7178e-02, -8.0332e-02, -7.8704e-02,\n",
       "         -4.8344e-02,  2.5099e-02, -2.4040e-02, -1.0734e-02,  8.7500e-03,\n",
       "         -4.2072e-02,  2.7883e-03, -2.3957e-02, -1.2086e-02, -6.4501e-03,\n",
       "         -3.4160e-02, -4.1683e-02, -4.5581e-02, -2.2742e-02,  6.7189e-02,\n",
       "         -4.3090e-02, -2.7423e-03,  6.6780e-02, -3.2529e-02,  5.7697e-02,\n",
       "         -3.4334e-02, -6.0007e-02,  7.9317e-02, -5.3668e-03,  4.4252e-03,\n",
       "         -1.1376e-02,  4.1125e-03, -1.9372e-02,  6.5942e-02,  6.8552e-02,\n",
       "          2.5084e-02,  1.2601e-02,  5.5814e-02, -2.4994e-02, -2.8228e-02,\n",
       "         -1.5365e-02,  3.2676e-02,  5.1524e-03,  3.2062e-02,  6.2384e-02,\n",
       "          1.9641e-02,  4.9283e-03, -3.2054e-02,  6.6147e-03, -1.0644e-01,\n",
       "         -4.1923e-02,  3.0136e-02,  7.6505e-02,  7.0891e-02, -1.6328e-02,\n",
       "          2.2507e-03, -5.3875e-02,  3.4208e-02,  8.1106e-02,  8.4940e-04,\n",
       "         -1.2404e-03, -1.0611e-02, -1.5435e-03, -7.8192e-02,  8.8597e-03,\n",
       "          6.2527e-03, -1.4953e-02,  6.4512e-03, -2.2099e-02,  5.7072e-03,\n",
       "         -9.4006e-02, -6.1185e-02,  9.5303e-02,  8.4248e-02, -3.4638e-02,\n",
       "          2.3156e-02, -2.8450e-02, -3.4871e-03, -5.0160e-02,  1.8463e-02,\n",
       "         -3.6356e-02,  8.0034e-02, -4.5968e-03,  1.3664e-02, -2.0722e-03,\n",
       "          2.7994e-02,  5.3747e-02,  4.5031e-02, -8.3082e-02,  3.7978e-02,\n",
       "         -7.5999e-02,  9.0973e-02, -3.6293e-02,  6.8599e-03, -8.5590e-03,\n",
       "          2.0768e-03, -1.0795e-02, -5.9023e-02,  1.7469e-03, -2.3478e-02,\n",
       "         -3.1956e-02,  1.2816e-02, -2.0861e-02,  7.8113e-03, -8.4299e-02,\n",
       "         -3.8310e-02,  2.0695e-02,  2.7052e-02, -3.4426e-03,  1.5750e-02,\n",
       "          7.4030e-03,  4.4968e-03,  4.6519e-03,  4.9720e-03, -3.9423e-02,\n",
       "         -1.9194e-02, -3.8924e-02,  1.2809e-02,  1.7032e-02, -6.8534e-02,\n",
       "          1.4460e-02, -4.8558e-02,  6.7321e-03, -5.8764e-02, -3.4996e-02,\n",
       "          6.3724e-02,  1.4518e-02,  3.6176e-02, -6.1307e-02,  1.3583e-03,\n",
       "          4.2515e-02, -6.2345e-02, -3.0441e-02, -3.0704e-02,  1.5775e-02,\n",
       "         -4.7089e-03, -1.4149e-03, -6.6226e-02,  4.0000e-03,  4.9728e-02,\n",
       "          1.2764e-02, -3.5197e-02,  1.0631e-02,  4.0535e-02,  4.4795e-02,\n",
       "         -1.0724e-02,  5.1562e-02,  6.3575e-02, -1.2379e-02, -1.8919e-02,\n",
       "          3.1763e-05, -8.9027e-02,  2.8042e-02, -1.8917e-02, -3.8486e-02,\n",
       "         -3.0836e-02, -6.4970e-02, -9.4138e-03, -5.2403e-02,  1.6799e-02,\n",
       "          1.0254e-01,  1.0793e-02, -1.6064e-02, -1.4511e-02,  1.4932e-02,\n",
       "          4.9705e-02,  4.0369e-02, -3.3406e-02, -8.2706e-03, -1.1410e-02,\n",
       "          6.2409e-02, -2.5138e-02, -2.8347e-02,  2.1202e-02,  1.9728e-02,\n",
       "          1.4329e-02, -2.9990e-02, -9.9683e-03,  4.5579e-02,  7.2237e-02,\n",
       "          1.4259e-02,  3.2941e-02,  4.0682e-02,  1.5637e-02,  1.0124e-02,\n",
       "          3.0548e-02,  9.7927e-03,  1.6593e-03, -9.9783e-03,  7.5896e-03,\n",
       "          5.3197e-02, -3.3863e-02, -5.9240e-02, -3.6340e-02, -6.4002e-02,\n",
       "          5.0860e-02,  1.1460e-02,  2.1225e-02,  2.0835e-02, -1.3057e-01,\n",
       "          8.7755e-03, -9.2588e-02, -1.5313e-02,  4.1757e-02,  6.3469e-02,\n",
       "         -1.0912e-02,  6.8530e-02,  6.9300e-02,  1.6294e-03, -2.7609e-02,\n",
       "         -5.2320e-02,  2.7525e-02,  1.7880e-02,  6.0804e-02, -3.3377e-02,\n",
       "          2.9208e-02,  6.3079e-03, -2.5470e-03,  2.2183e-02, -4.4711e-03,\n",
       "         -1.2323e-01, -4.4273e-02,  2.4568e-02,  9.9976e-03,  7.0003e-02,\n",
       "         -1.1652e-01,  4.4229e-02, -7.2017e-04, -4.3506e-02, -4.6241e-02,\n",
       "          3.8122e-02,  6.1618e-02, -6.9475e-03, -2.5975e-02,  1.2436e-02,\n",
       "          7.4180e-02,  2.0100e-02, -2.2468e-02, -4.4372e-02,  3.8313e-02,\n",
       "          5.5796e-02, -4.6234e-02, -2.5207e-02,  1.4491e-02, -7.1926e-02,\n",
       "         -1.2033e-02, -3.1446e-02,  2.7468e-02, -4.7897e-02, -7.3018e-03,\n",
       "          7.9147e-02,  1.3684e-02, -9.4294e-03,  7.7103e-02, -2.6797e-02,\n",
       "          1.7659e-02, -1.0753e-02, -1.9675e-02,  5.6504e-02,  1.9878e-04,\n",
       "         -3.1997e-02,  9.7774e-02,  1.1477e-02,  2.2747e-02, -6.4754e-02,\n",
       "          3.3931e-02,  5.9412e-02,  3.5183e-02, -5.1527e-02, -4.9860e-02,\n",
       "         -5.1822e-02, -5.5814e-02,  5.1007e-02, -2.3222e-02, -3.7696e-02,\n",
       "         -7.5508e-02, -9.8629e-03,  1.3449e-02,  8.2010e-03, -3.4040e-02,\n",
       "         -3.5188e-02,  2.9248e-02,  2.9076e-02,  2.1120e-03, -6.8708e-03,\n",
       "          2.3824e-02, -5.2956e-02, -6.1515e-02, -2.3878e-02,  4.9935e-03,\n",
       "          5.5031e-02, -4.5237e-03,  5.3007e-03, -4.9729e-03,  4.3007e-02,\n",
       "         -1.7544e-02, -2.8660e-02, -6.3067e-03, -4.0373e-02, -5.7926e-02,\n",
       "          3.9943e-02,  4.5591e-02, -8.3617e-03, -7.9746e-02,  3.2478e-02,\n",
       "         -1.1832e-02,  8.6142e-03,  3.4264e-02, -1.7025e-02, -8.8071e-02,\n",
       "         -5.2756e-02,  7.0290e-03,  7.8939e-02, -5.5074e-02,  5.9411e-02,\n",
       "          4.3280e-02, -2.0720e-02, -3.7859e-02, -6.6332e-03, -7.2177e-02,\n",
       "          2.0257e-02,  1.7229e-02, -4.0520e-02, -2.8291e-02,  5.1995e-02,\n",
       "          8.8287e-03,  7.6933e-02,  3.0171e-03,  5.5938e-02, -1.6244e-02,\n",
       "          6.8624e-02,  3.4599e-02,  1.5345e-02,  5.5102e-02,  7.6612e-02,\n",
       "         -1.5698e-02,  2.2012e-02, -8.0406e-02, -1.6143e-02, -6.8637e-03,\n",
       "         -4.8002e-02, -5.7430e-02, -2.3280e-02, -1.7647e-02, -2.3184e-04,\n",
       "          4.1284e-02, -1.3638e-02]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:25:17.145339Z",
     "start_time": "2025-06-22T10:25:17.112233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "def get_features_chunked(waveform: torch.Tensor, sampling_rate: int, chunk_size_seconds: float=0.25) -> torch.Tensor:\n",
    "    if len(waveform.shape) != 1:\n",
    "        raise ValueError(\"waveform should have shape [num_samples]\")\n",
    "    chunk_size = int(8 * ((sampling_rate * chunk_size_seconds) // 8))\n",
    "    all_features = []\n",
    "    for offset in tqdm(range(0, waveform.shape[0], chunk_size)):\n",
    "        chunk = waveform[offset:offset+chunk_size]\n",
    "        inputs = feature_extractor(chunk, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        print(inputs.keys())\n",
    "        audio_features = model.get_audio_features(input_features=inputs.input_features.to(device))\n",
    "        all_features.append(audio_features)\n",
    "        \n",
    "    return torch.stack(all_features)"
   ],
   "id": "5b4b82f01c2ca14b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:25:19.741282Z",
     "start_time": "2025-06-22T10:25:18.617877Z"
    }
   },
   "cell_type": "code",
   "source": "get_features_chunked(waveform[0], sampling_rate=sampling_rate)",
   "id": "c691f5b529b7e6c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2321e7ad43074572b09f9a75ba590ba5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_features', 'is_longer'])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_bicubic2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotImplementedError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mget_features_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mget_features_chunked\u001B[39m\u001B[34m(waveform, sampling_rate, chunk_size_seconds)\u001B[39m\n\u001B[32m     10\u001B[39m     inputs = feature_extractor(chunk, sampling_rate=sampling_rate, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m     \u001B[38;5;28mprint\u001B[39m(inputs.keys())\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     audio_features = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_audio_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m     all_features.append(audio_features)\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch.stack(all_features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/transformers/models/clap/modeling_clap.py:1920\u001B[39m, in \u001B[36mClapModel.get_audio_features\u001B[39m\u001B[34m(self, input_features, is_longer, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1915\u001B[39m output_hidden_states = (\n\u001B[32m   1916\u001B[39m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.output_hidden_states\n\u001B[32m   1917\u001B[39m )\n\u001B[32m   1918\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1920\u001B[39m audio_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maudio_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1921\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1922\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_longer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_longer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1923\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1924\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1926\u001B[39m pooled_output = audio_outputs[\u001B[32m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;28;01melse\u001B[39;00m audio_outputs.pooler_output\n\u001B[32m   1928\u001B[39m audio_features = \u001B[38;5;28mself\u001B[39m.audio_projection(pooled_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1510\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1511\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1515\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1516\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1517\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1518\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1519\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1520\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1522\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1523\u001B[39m     result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/transformers/models/clap/modeling_clap.py:1636\u001B[39m, in \u001B[36mClapAudioModel.forward\u001B[39m\u001B[34m(self, input_features, is_longer, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1631\u001B[39m output_attentions = output_attentions \u001B[38;5;28;01mif\u001B[39;00m output_attentions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.output_attentions\n\u001B[32m   1632\u001B[39m output_hidden_states = (\n\u001B[32m   1633\u001B[39m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.output_hidden_states\n\u001B[32m   1634\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1636\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maudio_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1637\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1638\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_longer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_longer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1639\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1640\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1641\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1642\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1510\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1511\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1515\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1516\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1517\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1518\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1519\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1520\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1522\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1523\u001B[39m     result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/transformers/models/clap/modeling_clap.py:906\u001B[39m, in \u001B[36mClapAudioEncoder.forward\u001B[39m\u001B[34m(self, input_features, is_longer, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001B[39m\n\u001B[32m    903\u001B[39m     is_longer_list = is_longer.to(input_features.device)\n\u001B[32m    904\u001B[39m     is_longer_list_idx = torch.where(is_longer_list == \u001B[32m1\u001B[39m)[\u001B[32m0\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m906\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreshape_mel2img\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormalized_input_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    908\u001B[39m frames_num = hidden_states.shape[\u001B[32m2\u001B[39m]\n\u001B[32m    910\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.patch_embed(hidden_states, is_longer_list_idx)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/transformers/models/clap/modeling_clap.py:865\u001B[39m, in \u001B[36mClapAudioEncoder.reshape_mel2img\u001B[39m\u001B[34m(self, normalized_input_features)\u001B[39m\n\u001B[32m    863\u001B[39m \u001B[38;5;66;03m# to avoid bicubic zero error\u001B[39;00m\n\u001B[32m    864\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m time_length < spec_width:\n\u001B[32m--> \u001B[39m\u001B[32m865\u001B[39m     normalized_input_features = \u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43minterpolate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnormalized_input_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbicubic\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m freq_length < spec_height:\n\u001B[32m    869\u001B[39m     normalized_input_features = nn.functional.interpolate(\n\u001B[32m    870\u001B[39m         normalized_input_features, (time_length, spec_height), mode=\u001B[33m\"\u001B[39m\u001B[33mbicubic\u001B[39m\u001B[33m\"\u001B[39m, align_corners=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    871\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/2.current/clapSlice/venv/lib/python3.12/site-packages/torch/nn/functional.py:4046\u001B[39m, in \u001B[36minterpolate\u001B[39m\u001B[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001B[39m\n\u001B[32m   4044\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m antialias:\n\u001B[32m   4045\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m torch._C._nn._upsample_bicubic2d_aa(\u001B[38;5;28minput\u001B[39m, output_size, align_corners, scale_factors)\n\u001B[32m-> \u001B[39m\u001B[32m4046\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupsample_bicubic2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_factors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4048\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m.dim() == \u001B[32m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mbilinear\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   4049\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mGot 3D input, but bilinear mode needs 4D input\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNotImplementedError\u001B[39m: The operator 'aten::upsample_bicubic2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33ba485e1b6afcc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
